{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import textwrap\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "import json\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    " \n",
    "import fire\n",
    "import torch\n",
    "from datasets import load_from_disk, load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pylab import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "cache_dir = \"/data/sambhav\" \n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\"\n",
    "    )\n",
    "\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013c8beab6d34d3993f4cf2515c12a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id,  quantization_config=bnb_config,torch_dtype = torch.bfloat16, cache_dir=cache_dir,device_map={'':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF_LEN = 4096\n",
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write an output that appropriately completes the request. \n",
    "### Instruction:\n",
    "Find the node degree of given node in the following input graph.\n",
    "### Input:\n",
    "{data_point[\"Input\"]}{data_point['Instruction']} \n",
    "### Output:\n",
    "{data_point[\"Output\"]}\"\"\"\n",
    "\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < CUTOFF_LEN\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    " \n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    " \n",
    "    return result\n",
    " \n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_prompt(data_point):\n",
    "    return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write an output that appropriately completes the request. \n",
    "### Instruction:\n",
    "Find the node degree of given node in the following input graph.\n",
    "### Input:\n",
    "{data_point[\"Input\"]}{data_point['Instruction']} \n",
    "### Output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,         ####Try 8\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 262410240 || all params: 3752071168 || trainable%: 6.993743675173274\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "        \n",
    "    )\n",
    "\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 21260288 || all params: 3773331456 || trainable%: 0.5634354746703705\n"
     ]
    }
   ],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# print_trainable_parameters(model)\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "MICRO_BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "LEARNING_RATE = 3e-4\n",
    "# TRAIN_STEPS = len(train_data)//BATCH_SIZE   ## 1 epoch  = 28K\n",
    "OUTPUT_DIR = \"/data/sambhav/LLM4Graph/experiments/Mistral/\"\n",
    "model.config.use_cache = False\n",
    "columns_to_remove = [\"Output\", \"Instruction\", \"Input\"]\n",
    "# data=load_dataset('json', data_files=file_path)['train']\n",
    "# train_data = (\n",
    "#         data.map(generate_and_tokenize_prompt)\n",
    "# )\n",
    "# train_data = train_data.remove_columns(columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1,   Dataset number: 0\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 28.00 MiB (GPU 2; 79.17 GiB total capacity; 624.84 MiB already allocated; 20.88 MiB free; 626.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/data/sambhav/LLM4Graph/training_scripts/bitsandbytes/node_degree.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdive7.engr.tamu.edu/data/sambhav/LLM4Graph/training_scripts/bitsandbytes/node_degree.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m data_collator \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39mDataCollatorForSeq2Seq(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdive7.engr.tamu.edu/data/sambhav/LLM4Graph/training_scripts/bitsandbytes/node_degree.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     tokenizer, pad_to_multiple_of\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdive7.engr.tamu.edu/data/sambhav/LLM4Graph/training_scripts/bitsandbytes/node_degree.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdive7.engr.tamu.edu/data/sambhav/LLM4Graph/training_scripts/bitsandbytes/node_degree.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m trainer \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdive7.engr.tamu.edu/data/sambhav/LLM4Graph/training_scripts/bitsandbytes/node_degree.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdive7.engr.tamu.edu/data/sambhav/LLM4Graph/training_scripts/bitsandbytes/node_degree.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mtrain_data,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdive7.engr.tamu.edu/data/sambhav/LLM4Graph/training_scripts/bitsandbytes/node_degree.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_arguments,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdive7.engr.tamu.edu/data/sambhav/LLM4Graph/training_scripts/bitsandbytes/node_degree.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m     data_collator\u001b[39m=\u001b[39mdata_collator\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdive7.engr.tamu.edu/data/sambhav/LLM4Graph/training_scripts/bitsandbytes/node_degree.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdive7.engr.tamu.edu/data/sambhav/LLM4Graph/training_scripts/bitsandbytes/node_degree.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m/data/sambhav/miniconda3/envs/mistral/lib/python3.8/site-packages/transformers/trainer.py:1506\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1504\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1505\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1506\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1507\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1508\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1509\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1510\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1511\u001b[0m     )\n",
      "File \u001b[0;32m/data/sambhav/miniconda3/envs/mistral/lib/python3.8/site-packages/transformers/trainer.py:1801\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1798\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1800\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1801\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1803\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1804\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1805\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1806\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1807\u001b[0m ):\n\u001b[1;32m   1808\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1809\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/data/sambhav/miniconda3/envs/mistral/lib/python3.8/site-packages/transformers/trainer.py:2650\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2647\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2649\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2650\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2652\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2653\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/data/sambhav/miniconda3/envs/mistral/lib/python3.8/site-packages/transformers/trainer.py:2673\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2671\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2672\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2673\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2674\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2675\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2676\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/data/sambhav/miniconda3/envs/mistral/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/sambhav/miniconda3/envs/mistral/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:170\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[0;32m--> 170\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplicate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids[:\u001b[39mlen\u001b[39;49m(inputs)])\n\u001b[1;32m    171\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel_apply(replicas, inputs, kwargs)\n\u001b[1;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather(outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m/data/sambhav/miniconda3/envs/mistral/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:175\u001b[0m, in \u001b[0;36mDataParallel.replicate\u001b[0;34m(self, module, device_ids)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreplicate\u001b[39m(\u001b[39mself\u001b[39m, module, device_ids):\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m replicate(module, device_ids, \u001b[39mnot\u001b[39;49;00m torch\u001b[39m.\u001b[39;49mis_grad_enabled())\n",
      "File \u001b[0;32m/data/sambhav/miniconda3/envs/mistral/lib/python3.8/site-packages/torch/nn/parallel/replicate.py:91\u001b[0m, in \u001b[0;36mreplicate\u001b[0;34m(network, devices, detach)\u001b[0m\n\u001b[1;32m     89\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(network\u001b[39m.\u001b[39mparameters())\n\u001b[1;32m     90\u001b[0m param_indices \u001b[39m=\u001b[39m {param: idx \u001b[39mfor\u001b[39;00m idx, param \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(params)}\n\u001b[0;32m---> 91\u001b[0m param_copies \u001b[39m=\u001b[39m _broadcast_coalesced_reshape(params, devices, detach)\n\u001b[1;32m     93\u001b[0m buffers \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(network\u001b[39m.\u001b[39mbuffers())\n\u001b[1;32m     94\u001b[0m buffers_rg \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/data/sambhav/miniconda3/envs/mistral/lib/python3.8/site-packages/torch/nn/parallel/replicate.py:71\u001b[0m, in \u001b[0;36m_broadcast_coalesced_reshape\u001b[0;34m(tensors, devices, detach)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[39m# Use the autograd function to broadcast if not detach\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(tensors) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 71\u001b[0m         tensor_copies \u001b[39m=\u001b[39m Broadcast\u001b[39m.\u001b[39;49mapply(devices, \u001b[39m*\u001b[39;49mtensors)\n\u001b[1;32m     72\u001b[0m         \u001b[39mreturn\u001b[39;00m [tensor_copies[i:i \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(tensors)]\n\u001b[1;32m     73\u001b[0m                 \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(tensor_copies), \u001b[39mlen\u001b[39m(tensors))]\n\u001b[1;32m     74\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/data/sambhav/miniconda3/envs/mistral/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:23\u001b[0m, in \u001b[0;36mBroadcast.forward\u001b[0;34m(ctx, target_gpus, *inputs)\u001b[0m\n\u001b[1;32m     21\u001b[0m ctx\u001b[39m.\u001b[39mnum_inputs \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(inputs)\n\u001b[1;32m     22\u001b[0m ctx\u001b[39m.\u001b[39minput_device \u001b[39m=\u001b[39m inputs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_device()\n\u001b[0;32m---> 23\u001b[0m outputs \u001b[39m=\u001b[39m comm\u001b[39m.\u001b[39;49mbroadcast_coalesced(inputs, ctx\u001b[39m.\u001b[39;49mtarget_gpus)\n\u001b[1;32m     24\u001b[0m non_differentiables \u001b[39m=\u001b[39m []\n\u001b[1;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m idx, input_requires_grad \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(ctx\u001b[39m.\u001b[39mneeds_input_grad[\u001b[39m1\u001b[39m:]):\n",
      "File \u001b[0;32m/data/sambhav/miniconda3/envs/mistral/lib/python3.8/site-packages/torch/nn/parallel/comm.py:58\u001b[0m, in \u001b[0;36mbroadcast_coalesced\u001b[0;34m(tensors, devices, buffer_size)\u001b[0m\n\u001b[1;32m     56\u001b[0m devices \u001b[39m=\u001b[39m [_get_device_index(d) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m devices]\n\u001b[1;32m     57\u001b[0m tensors \u001b[39m=\u001b[39m [_handle_complex(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m tensors]\n\u001b[0;32m---> 58\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_broadcast_coalesced(tensors, devices, buffer_size)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 2; 79.17 GiB total capacity; 624.84 MiB already allocated; 20.88 MiB free; 626.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    dataset_index = epoch%78\n",
    "    print(f\"Epoch number: {(epoch//78) +1},   Dataset number: {dataset_index}\")\n",
    "    file_path = f\"/mnt/data/shared/sambhav/node_degree_partial_dataset_{dataset_index}.json\"\n",
    "    data=load_dataset('json', data_files=file_path)['train']\n",
    "\n",
    "    \n",
    "    \n",
    "    train_data = (\n",
    "        data.map(generate_and_tokenize_prompt)\n",
    "    )\n",
    "    train_data = train_data.remove_columns(columns_to_remove)\n",
    "    \n",
    "    training_arguments = transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "        # per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        warmup_steps=100,\n",
    "        # max_steps=TRAIN_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        fp16=True,\n",
    "        # logging_steps=10000,\n",
    "        optim=\"adamw_torch\",\n",
    "        num_train_epochs=1,\n",
    "        # evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        # eval_steps=20000,\n",
    "        # save_steps=20000,\n",
    "        output_dir=OUTPUT_DIR+f\"step_{epoch}\",\n",
    "        # load_best_model_at_end=True,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "    )\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        args=training_arguments,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = transformers.Trainer(\n",
    "#     model=model,\n",
    "#     train_dataset=train_data,\n",
    "#     args=training_arguments,\n",
    "#     data_collator=data_collator\n",
    "# )\n",
    "\n",
    "# # old_state_dict = model.state_dict\n",
    "# # model.state_dict = (\n",
    "# #     lambda self, *_, **__: get_peft_model_state_dict(\n",
    "# #         self, old_state_dict()\n",
    "# #     )\n",
    "# # ).__get__(model, type(model))\n",
    " \n",
    "# # model = torch.compile(model)\n",
    " \n",
    "# trainer.train()\n",
    "# # model.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prompt=generate_test_prompt(train_data[0])\n",
    "# prompt=\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write an output that appropriately completes the request. \n",
    "# ### Instruction:\n",
    "# Find the node degree of given node in the following input graph.\n",
    "# ### Input:\n",
    "# An undirected graph has 45 nodes - 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43 and 44.\n",
    "# Node 0 is connected to nodes 2, 5, 6, 7, 16, 19, 21, 23, 27, 28, 29, 34, 35, 40 and 42.\n",
    "# Node 1 is connected to nodes 3, 5, 6, 7, 9, 10, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 31, 33, 34, 35, 36, 37, 38, 40 and 44.\n",
    "# Node 2 is connected to nodes 0, 4, 6, 7, 8, 9, 12, 14, 15, 20, 22, 23, 24, 25, 26, 28, 29, 30, 40 and 41.\n",
    "# Node 3 is connected to nodes 1, 5, 6, 9, 10, 14, 15, 16, 18, 19, 21, 22, 23, 24, 25, 28, 30, 34, 38 and 43.\n",
    "# Node 4 is connected to nodes 2, 6, 7, 8, 9, 10, 12, 13, 17, 18, 19, 21, 22, 23, 24, 25, 29, 31, 32, 33, 36, 37 and 42.\n",
    "# Node 5 is connected to nodes 0, 1, 3, 6, 7, 9, 10, 11, 12, 15, 17, 19, 20, 21, 22, 25, 26, 27, 28, 30, 31, 33, 35, 36, 38, 39, 40, 41, 42, 43 and 44.\n",
    "# Node 6 is connected to nodes 0, 1, 2, 3, 4, 5, 8, 9, 10, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 24, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 42, 43 and 44.\n",
    "# Node 7 is connected to nodes 0, 1, 2, 4, 5, 8, 10, 15, 16, 20, 21, 23, 25, 26, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 41 and 43.\n",
    "# Node 8 is connected to nodes 2, 4, 6, 7, 14, 15, 16, 20, 22, 24, 32, 33, 35, 38 and 39.\n",
    "# Node 9 is connected to nodes 1, 2, 3, 4, 5, 6, 14, 15, 18, 22, 26, 35, 38, 40 and 44.\n",
    "# Node 10 is connected to nodes 1, 3, 4, 5, 6, 7, 14, 15, 16, 19, 20, 21, 22, 23, 24, 30, 31, 33, 36, 37, 38, 42 and 43.\n",
    "# Node 11 is connected to nodes 5, 15, 16, 19, 21, 22, 23, 25, 28, 31, 32, 33, 37, 38 and 44.\n",
    "# Node 12 is connected to nodes 1, 2, 4, 5, 16, 17, 18, 20, 21, 25, 31, 33, 34, 38, 41 and 42.\n",
    "# Node 13 is connected to nodes 1, 4, 6, 15, 18, 21, 23, 24, 25, 29, 38, 41, 42 and 44.\n",
    "# Node 14 is connected to nodes 1, 2, 3, 6, 8, 9, 10, 15, 16, 21, 22, 23, 25, 28, 29, 31, 32, 33, 35, 36, 37, 38, 39 and 44.\n",
    "# Node 15 is connected to nodes 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 13, 14, 16, 19, 21, 24, 28, 29, 31, 32, 33, 35, 37, 38, 39, 40 and 43.\n",
    "# Node 16 is connected to nodes 0, 1, 3, 6, 7, 8, 10, 11, 12, 14, 15, 17, 18, 19, 20, 21, 22, 23, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 40 and 44.\n",
    "# Node 17 is connected to nodes 4, 5, 6, 12, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29, 31, 36 and 42.\n",
    "# Node 18 is connected to nodes 1, 3, 4, 9, 12, 13, 16, 17, 22, 23, 24, 25, 28, 31, 33, 41 and 43.\n",
    "# Node 19 is connected to nodes 0, 1, 3, 4, 5, 6, 10, 11, 15, 16, 20, 21, 22, 24, 27, 30, 34, 35, 36, 38 and 42.\n",
    "# Node 20 is connected to nodes 1, 2, 5, 6, 7, 8, 10, 12, 16, 19, 21, 22, 23, 27, 28, 30, 31, 34, 40, 42 and 43.\n",
    "# Node 21 is connected to nodes 0, 1, 3, 4, 5, 6, 7, 10, 11, 12, 13, 14, 15, 16, 19, 20, 24, 25, 26, 28, 29, 31, 34, 36, 38 and 44.\n",
    "# Node 22 is connected to nodes 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 14, 16, 17, 18, 19, 20, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43 and 44.\n",
    "# Node 23 is connected to nodes 0, 2, 3, 4, 6, 7, 10, 11, 13, 14, 16, 17, 18, 20, 22, 24, 25, 26, 27, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39 and 43.\n",
    "# Node 24 is connected to nodes 1, 2, 3, 4, 6, 8, 10, 13, 15, 17, 18, 19, 21, 22, 23, 25, 26, 28, 30, 31, 32, 33, 35, 36, 38, 40 and 42.\n",
    "# Node 25 is connected to nodes 1, 2, 3, 4, 5, 7, 11, 12, 13, 14, 17, 18, 21, 22, 23, 24, 28, 29, 30, 33, 35, 36, 38, 39, 41, 42, 43 and 44.\n",
    "# Node 26 is connected to nodes 1, 2, 5, 7, 9, 16, 17, 21, 22, 23, 24, 27, 28, 29, 31, 32, 33, 35, 36, 41, 42 and 44.\n",
    "# Node 27 is connected to nodes 0, 1, 5, 6, 16, 17, 19, 20, 22, 23, 26, 28, 29, 31, 35, 38 and 42.\n",
    "# Node 28 is connected to nodes 0, 1, 2, 3, 5, 6, 7, 11, 14, 15, 16, 17, 18, 20, 21, 22, 24, 25, 26, 27, 29, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41 and 44.\n",
    "# Node 29 is connected to nodes 0, 1, 2, 4, 6, 7, 13, 14, 15, 16, 17, 21, 22, 23, 25, 26, 27, 28, 30, 31, 33, 36, 38, 39 and 41.\n",
    "# Node 30 is connected to nodes 2, 3, 5, 6, 7, 10, 16, 19, 20, 22, 24, 25, 29, 33, 36, 38, 42 and 44.\n",
    "# Node 31 is connected to nodes 1, 4, 5, 6, 7, 10, 11, 12, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 26, 27, 28, 29, 32, 33, 37, 38, 40 and 44.\n",
    "# Node 32 is connected to nodes 4, 6, 7, 8, 11, 14, 15, 16, 22, 23, 24, 26, 31, 36, 39 and 41.\n",
    "# Node 33 is connected to nodes 1, 4, 5, 6, 7, 8, 10, 11, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 28, 29, 30, 31, 34, 36, 37, 38, 40 and 43.\n",
    "# Node 34 is connected to nodes 0, 1, 3, 6, 12, 16, 19, 20, 21, 23, 28, 33, 36 and 38.\n",
    "# Node 35 is connected to nodes 0, 1, 5, 6, 7, 8, 9, 14, 15, 16, 19, 22, 23, 24, 25, 26, 27, 28, 36, 38, 41, 42, 43 and 44.\n",
    "# Node 36 is connected to nodes 1, 4, 5, 6, 7, 10, 14, 16, 17, 19, 21, 22, 23, 24, 25, 26, 28, 29, 30, 32, 33, 34, 35, 38, 39, 40, 41 and 44.\n",
    "# Node 37 is connected to nodes 1, 4, 6, 7, 10, 11, 14, 15, 22, 23, 28, 31, 33 and 44.\n",
    "# Node 38 is connected to nodes 1, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 19, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 33, 34, 35, 36, 41, 42, 43 and 44.\n",
    "# Node 39 is connected to nodes 5, 6, 7, 8, 14, 15, 22, 23, 25, 28, 29, 32, 36 and 44.\n",
    "# Node 40 is connected to nodes 0, 1, 2, 5, 9, 15, 16, 20, 22, 24, 28, 31, 33 and 36.\n",
    "# Node 41 is connected to nodes 2, 5, 7, 12, 13, 18, 22, 25, 26, 28, 29, 32, 35, 36, 38, 42 and 44.\n",
    "# Node 42 is connected to nodes 0, 4, 5, 6, 10, 12, 13, 17, 19, 20, 22, 24, 25, 26, 27, 30, 35, 38 and 41.\n",
    "# Node 43 is connected to nodes 3, 5, 6, 7, 10, 15, 18, 20, 22, 23, 25, 33, 35, 38 and 44.\n",
    "# Node 44 is connected to nodes 1, 5, 6, 9, 11, 13, 14, 16, 21, 22, 25, 26, 28, 30, 31, 35, 36, 37, 38, 39, 41 and 43.\n",
    "# What is the degree of Node 20? \n",
    "# ### Output:\"\"\"\n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_input=tokenizer(prompt, return_tensors = \"pt\")\n",
    "\n",
    "# device=\"cuda:0\"\n",
    "# val_input.to(device)\n",
    "# ft_model.eval()\n",
    "# with torch.no_grad():\n",
    "#     generated_ids = ft_model.generate(**val_input, max_new_tokens=10, pad_token_id=tokenizer.eos_token_id)\n",
    "# decoded = tokenizer.batch_decode(generated_ids)\n",
    "# print(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_dir = \"/data/sambhav/LLM4Graph/experiments/Mistral/checkpoint-100/\"\n",
    "# from peft import PeftModel\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(base_model_id,  quantization_config=bnb_config,torch_dtype = torch.bfloat16, cache_dir=cache_dir,device_map={'':0})\n",
    "# print_trainable_parameters(base_model)\n",
    "# ft_model = PeftModel.from_pretrained(base_model,checkpoint_dir)\n",
    "# print_trainable_parameters(ft_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
